{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "# Predictions\n\nIn this notebook you will build a link prediction classifier using Neo4j and scikit-learn. \n\nExecute the code to import the libraries (remember to unset Reset all runtimes before running):"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "from py2neo import Graph\nimport pandas as pd\n\nimport matplotlib \nimport matplotlib.pyplot as plt\n\nplt.style.use(\u0027fivethirtyeight\u0027)\npd.set_option(\u0027display.float_format\u0027, lambda x: \u0027%.3f\u0027 % x)\npd.set_option(\u0027display.max_colwidth\u0027, 100)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Next, create a connection to your Neo4j Sandbox, just as you did previously when you set up your environment. \n\n\u003cdiv align\u003d\"left\"\u003e\n    \u003cimg src\u003d\"images/sandbox-citations.png\" alt\u003d\"Citation Sandbox\"/\u003e\n\u003c/div\u003e\n\nUpdate the cell below to use the IP Address, Bolt Port, and Password, as you did previously."
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "# Change the line of code below to use the IP Address, Bolt Port, and Password of your Sandbox.\n# graph \u003d Graph(\"bolt://\u003cIP Address\u003e:\u003cBolt Port\u003e\", auth\u003d(\"neo4j\", \"\u003cPassword\u003e\")) \n \ngraph \u003d Graph(\"bolt://52.3.242.176:33698\", auth\u003d(\"neo4j\", \"equivalent-listing-parts\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "## Building a co-author graph\n\nYou will build an inferred graph of co-authors based on people collaborating on the same papers. You will store a property on the relationship indicating the year of their first collaboration.\n\nRun this code to add the year data to the relationships:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "query \u003d \"\"\"\n",
        "CALL apoc.periodic.iterate(\n",
        "  \"MATCH (a1)\u003c-[:AUTHOR]-(paper)-[:AUTHOR]-\u003e(a2:Author)\n",
        "   WITH a1, a2, paper\n",
        "   ORDER BY a1, paper.year\n",
        "   RETURN a1, a2, collect(paper)[0].year AS year, count(*) AS collaborations\",\n",
        "  \"MERGE (a1)-[coauthor:CO_AUTHOR {year: year}]-(a2)\n",
        "   SET coauthor.collaborations \u003d collaborations\", \n",
        "  {batchSize: 100})\n",
        "\"\"\"\n",
        "\n",
        "graph.run(query).stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Now that you have modified the co-author graph, you want an approach that will allow you to predict future links (relationships) that will be created between people. \n\nYou will use the [Link Prediction algorithms](https://neo4j.com/docs/graph-algorithms/current/algorithms/linkprediction/) that you just learned about in the previous section.  After you have computed scores with these algorithms what should you do?\n\nThere are two main approaches that one can take:\n\n### Using the measures directly\n\nYou can use the scores from the Link Predictions directly, specifying a __threshold value__ above which we predict that a link will be created between two nodes.\n\n### Supervised learning\n\nUsing the supervised learning approach, you use the scores as features to train a binary classifier. The binary classifier then predicts whether a pair of nodes will have a link.\n\nIn this notebook you will use the supervised learning approach."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "## Train and test datasets \n\nFirst, you need to come up with train and test datasets on which you can build, and then evaluate a model.\n\n### Positive examples\n\nThe tricky thing when working with graph data is that you cannot just randomly split the data, as this could lead to data leakage.\n\nData leakage can occur when data outside of your training data is inadvertently used to create your model. This can easily happen when working with graphs because pairs of nodes in your training set may be connected to those in the test set.\n\nWhen we compute Link Prediction measures over that training set the __measures computed contain information from the test set__ that we’ll later evaluate our model against.\n\nInstead, you need to split your graph into training and test sub graphs. If your graph has a concept of time, it is easy — we can split the graph at a point in time and the training set will be from before the time, and the test set will be after.\n\nThis is still not a perfect solution and you will need to try and ensure that the general network structure in the training and test sub graphs is similar.\n\nOnce you have done that you will have pairs of nodes in the train and test set that have relationships between them. They will be the __positive examples__ in our machine learning model.\n\nWe are lucky that our citation graph contains a time value. You can create train and test graphs by splitting the data on a particular year. First, you need to figure out what year that should be. Let\u0027s look at the distribution of the first year that co-authors collaborated:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "query \u003d \"\"\"\n",
        "MATCH p\u003d()-[r:CO_AUTHOR]-\u003e() \n",
        "WITH r.year AS year, count(*) AS count\n",
        "ORDER BY year\n",
        "RETURN toString(year) AS year, count\n",
        "\"\"\"\n",
        "by_year \u003d graph.run(query).to_data_frame()\n",
        "\n",
        "ax \u003d by_year.plot(kind\u003d\u0027bar\u0027, x\u003d\u0027year\u0027, y\u003d\u0027count\u0027, legend\u003dNone, figsize\u003d(15,8))\n",
        "ax.xaxis.set_label_text(\"\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "It looks like 2006 would act as a good year to split the data. You will take all the co-authorships from 2005 and earlier as the train graph, and everything from 2006 onwards as the test graph.\n\nCreate explicit `CO_AUTHOR_EARLY` and `CO_AUTHOR_LATE` relationships in the graph based on that year by executing this code:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "query \u003d \"\"\"\n",
        "MATCH (a)-[r:CO_AUTHOR]-\u003e(b) \n",
        "where r.year \u003c 2006\n",
        "MERGE (a)-[:CO_AUTHOR_EARLY {year: r.year}]-(b);\n",
        "\"\"\"\n",
        "\n",
        "graph.run(query).stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "query \u003d \"\"\"\n",
        "MATCH (a)-[r:CO_AUTHOR]-\u003e(b) \n",
        "where r.year \u003e\u003d 2006\n",
        "MERGE (a)-[:CO_AUTHOR_LATE {year: r.year}]-(b);\n",
        "\"\"\"\n",
        "\n",
        "graph.run(query).stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Check how many co-author relationship we have in each of these sub graphs:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "query \u003d \"\"\"\n",
        "MATCH ()-[:CO_AUTHOR_EARLY]-\u003e()\n",
        "RETURN count(*) AS count\n",
        "\"\"\"\n",
        "\n",
        "graph.run(query).to_data_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "query \u003d \"\"\"\n",
        "MATCH ()-[:CO_AUTHOR_LATE]-\u003e()\n",
        "RETURN count(*) AS count\n",
        "\"\"\"\n",
        "\n",
        "graph.run(query).to_data_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "We have a split of 52-48, which is a bit on the high side, but should be ok. Next, we create the __negative examples__.\n\n### Negative examples\n\nThe simplest approach would be to use all pair of nodes that don’t have a relationship. __The problem with this approach is that there are significantly more examples of pairs of nodes that don’t have a relationship than there are pairs of nodes that do__.\n\nThe maximum number of negative examples is equal to:\n\n```\n# negative examples \u003d (# nodes)² - (# relationships) - (# nodes)\n```\n\ni.e. the number of nodes squared, minus the relationships that the graph has, minus self relationships.\n\nIf you use all of these negative examples in the training set you will have a massive class imbalance — there are many negative examples and relatively few positive ones.\n\nA model trained using data that’s this imbalanced will achieve very high accuracy by __predicting that any pair of nodes don’t have a relationship__ between them, which is not quite what we want!\n\nSo we need to try and reduce the number of negative examples. An approach described in several link prediction papers is to use pairs of nodes that are a __specific number of hops away from each other__.\n\nThis will significantly reduce the number of negative examples, although there will still be a lot more negative examples than positive.\n\nTo solve this problem, you either need to down sample the negative examples or up sample the positive examples.\n\nYou will take the down sampling approach in this exercise. Execute this code reduce the number of negative examples:"
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "source": "def down_sample(df):\n    copy \u003d df.copy()\n    zero \u003d Counter(copy.label.values)[0]\n    un \u003d Counter(copy.label.values)[1]\n    n \u003d zero - un\n    copy \u003d copy.drop(copy[copy.label \u003d\u003d 0].sample(n\u003dn, random_state\u003d1).index)\n    return copy.sample(frac\u003d1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Now you are ready to build the train and test datasets based on the train and test sub graphs that you created. \n\n* The positive examples will be taken directly from the graph. \n* The negative examples will be found by looking for people who are 2 or 3 hops away from each other, excluding those that have already collaborated. We\u0027ll then down sample those examples to equal the size of the positive examples. \n\nExecute this code the build the train and test datasets:"
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": "train_existing_links \u003d graph.run(\"\"\"\nMATCH (author:Author)-[:CO_AUTHOR_EARLY]-\u003e(other:Author)\nRETURN id(author) AS node1, id(other) AS node2, 1 AS label\n\"\"\").to_data_frame()\n\ntrain_missing_links \u003d graph.run(\"\"\"\nMATCH (author:Author)\nWHERE (author)-[:CO_AUTHOR_EARLY]-()\nMATCH (author)-[:CO_AUTHOR_EARLY*2..3]-(other)\nWHERE not((author)-[:CO_AUTHOR_EARLY]-(other))\nRETURN id(author) AS node1, id(other) AS node2, 0 AS label\n\"\"\").to_data_frame()\ntrain_missing_links \u003d train_missing_links.drop_duplicates()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "training_df \u003d train_missing_links.append(train_existing_links, ignore_index\u003dTrue)\ntraining_df[\u0027label\u0027] \u003d training_df[\u0027label\u0027].astype(\u0027category\u0027)\ntraining_df \u003d down_sample(training_df)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Execute this code to see what the train DataFrame contains:"
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": "training_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Repeat the process for the test set:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "test_existing_links \u003d graph.run(\"\"\"\nMATCH (author:Author)-[:CO_AUTHOR_LATE]-\u003e(other:Author)\nRETURN id(author) AS node1, id(other) AS node2, 1 AS label\n\"\"\").to_data_frame()\n\ntest_missing_links \u003d graph.run(\"\"\"\nMATCH (author:Author)\nWHERE (author)-[:CO_AUTHOR_LATE]-()\nMATCH (author)-[:CO_AUTHOR_LATE*2..3]-(other)\nWHERE not((author)-[:CO_AUTHOR_LATE]-(other))\nRETURN id(author) AS node1, id(other) AS node2, 0 AS label\n\"\"\").to_data_frame()\ntest_missing_links \u003d test_missing_links.drop_duplicates()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "test_df \u003d test_missing_links.append(test_existing_links, ignore_index\u003dTrue)\ntest_df[\u0027label\u0027] \u003d test_df[\u0027label\u0027].astype(\u0027category\u0027)\ntest_df \u003d down_sample(test_df)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Execute this code to see what the test DataFrame contains:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "# Choosing a machine learning algorithm\n\nWe will create our machine learning pipeline based on a Random Forest classifier. This method is well suited as our data set will be comprised of a mix of strong and weak features. While the weak features will sometimes be helpful, the Random Forest method will ensure we don’t create a model that only fits our training data."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "classifier \u003d RandomForestClassifier(n_estimators\u003d30, max_depth\u003d10, random_state\u003d0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "# Generating graphy features\n\nNext, we create a simple model that tries to predict whether two authors will have a future collaboration based on features extracted from common authors, preferential attachment, and the total union of neighbors.\n\nThe following function computes each of these measures for pairs of nodes:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def apply_graphy_features(data, rel_type):\n",
        "    query \u003d \"\"\"\n",
        "    UNWIND $pairs AS pair\n",
        "    MATCH (p1) WHERE id(p1) \u003d pair.node1\n",
        "    MATCH (p2) WHERE id(p2) \u003d pair.node2\n",
        "    RETURN pair.node1 AS node1,\n",
        "           pair.node2 AS node2,\n",
        "           algo.linkprediction.commonNeighbors(\n",
        "               p1, p2, {relationshipQuery: $relType}) AS cn,\n",
        "           algo.linkprediction.preferentialAttachment(\n",
        "               p1, p2, {relationshipQuery: $relType}) AS pa,\n",
        "           algo.linkprediction.totalNeighbors(\n",
        "               p1, p2, {relationshipQuery: $relType}) AS tn\n",
        "    \"\"\"\n",
        "    pairs \u003d [{\"node1\": node1, \"node2\": node2}  for node1,node2 in data[[\"node1\", \"node2\"]].values.tolist()]\n",
        "    features \u003d graph.run(query, {\"pairs\": pairs, \"relType\": rel_type}).to_data_frame()\n",
        "    return pd.merge(data, features, on \u003d [\"node1\", \"node2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Apply this function to the training DataFrame:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "training_df \u003d apply_graphy_features(training_df, \"CO_AUTHOR_EARLY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "This is what the DataFrame looks like now:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "training_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Do the same to the test DataFrame:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "test_df \u003d apply_graphy_features(test_df, \"CO_AUTHOR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Next, you will build a model based on these graphy features. Start by just using one of the features - common neighbors. \n\nThe following code builds a Random Forest model, evaluates it against the test dataset, and then indicates which of the features had the most importance in the model."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "columns \u003d [\"cn\"]\n",
        "\n",
        "X \u003d training_df[columns]\n",
        "y \u003d training_df[\"label\"]\n",
        "classifier.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Next, you need to evaluate the model. You will compute its accuracy, precision, and recall. Then, you will return the importance of each feature used in the model. The following functions will help with this:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def evaluate_model(predictions, actual):\n",
        "    return pd.DataFrame({\n",
        "        \"Measure\": [\"Accuracy\", \"Precision\", \"Recall\"],\n",
        "        \"Score\": [accuracy_score(actual, predictions), \n",
        "                  precision_score(actual, predictions), \n",
        "                  recall_score(actual, predictions)]\n",
        "    })\n",
        "\n",
        "def feature_importance(columns, classifier):        \n",
        "    display(\"Feature Importance\")\n",
        "    df \u003d pd.DataFrame({\n",
        "        \"Feature\": columns,\n",
        "        \"Importance\": classifier.feature_importances_\n",
        "    })\n",
        "    df \u003d df.sort_values(\"Importance\", ascending\u003dFalse)    \n",
        "    ax \u003d df.plot(kind\u003d\u0027bar\u0027, x\u003d\u0027Feature\u0027, y\u003d\u0027Importance\u0027, legend\u003dNone)\n",
        "    ax.xaxis.set_label_text(\"\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "predictions \u003d classifier.predict(test_df[columns])\n",
        "y_test \u003d test_df[\"label\"]\n",
        "\n",
        "display(evaluate_model(predictions, y_test))\n",
        "feature_importance(columns, classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "The scores for accuracy and precision aren\u0027t bad, but our recall isn\u0027t very good. What if you include preferential attachment and total neighbors as well:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "columns \u003d [\"cn\", \"pa\", \"tn\"]\n",
        "\n",
        "X \u003d training_df[columns]\n",
        "y \u003d training_df[\"label\"]\n",
        "classifier.fit(X, y)\n",
        "\n",
        "predictions \u003d classifier.predict(test_df[columns])\n",
        "y_test \u003d test_df[\"label\"]\n",
        "\n",
        "display(evaluate_model(predictions, y_test))\n",
        "feature_importance(columns, classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Common Neighbors is the dominant feature, but including the two other features has improved the accuracy and recall of the model.\n\nNext, you will add some new features that are generated from graph algorithms."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "# Triangles and The Clustering Coefficient\n\nStart by running the [triangle count](https://neo4j.com/docs/graph-algorithms/current/algorithms/triangle-counting-clustering-coefficient/) algorithm over the test and train sub graphs. This algorithm will return the number of triangles that each node forms, as well as each node\u0027s clustering coefficient. The clustering coefficient of a node indicates the likelihood that its neighbors are also connected."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.triangleCount(\u0027Author\u0027, \u0027CO_AUTHOR_EARLY\u0027, { write:true,\n",
        "writeProperty:\u0027trianglesTrain\u0027, clusteringCoefficientProperty:\u0027coefficientTrain\u0027});\n",
        "\"\"\").to_data_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.triangleCount(\u0027Author\u0027, \u0027CO_AUTHOR\u0027, { write:true,\n",
        "writeProperty:\u0027trianglesTest\u0027, clusteringCoefficientProperty:\u0027coefficientTest\u0027});\n",
        "\"\"\").to_data_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "The following function will add these features to the train and test DataFrames:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def apply_triangles_features(data, triangles_prop, coefficient_prop):\n",
        "    query \u003d \"\"\"\n",
        "    UNWIND $pairs AS pair\n",
        "    MATCH (p1) WHERE id(p1) \u003d pair.node1\n",
        "    MATCH (p2) WHERE id(p2) \u003d pair.node2\n",
        "    RETURN pair.node1 AS node1,\n",
        "    pair.node2 AS node2,\n",
        "    apoc.coll.min([p1[$trianglesProp], p2[$trianglesProp]]) AS minTriangles,\n",
        "    apoc.coll.max([p1[$trianglesProp], p2[$trianglesProp]]) AS maxTriangles,\n",
        "    apoc.coll.min([p1[$coefficientProp], p2[$coefficientProp]]) AS minCoefficient,\n",
        "    apoc.coll.max([p1[$coefficientProp], p2[$coefficientProp]]) AS maxCoefficient\n",
        "    \"\"\"    \n",
        "    pairs \u003d [{\"node1\": node1, \"node2\": node2}  for node1,node2 in data[[\"node1\", \"node2\"]].values.tolist()]    \n",
        "    params \u003d {\n",
        "    \"pairs\": pairs,\n",
        "    \"trianglesProp\": triangles_prop,\n",
        "    \"coefficientProp\": coefficient_prop\n",
        "    }\n",
        "    features \u003d graph.run(query, params).to_data_frame()    \n",
        "    return pd.merge(data, features, on \u003d [\"node1\", \"node2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Add these new features:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "training_df \u003d apply_triangles_features(training_df, \"trianglesTrain\", \"coefficientTrain\")\n",
        "test_df \u003d apply_triangles_features(test_df, \"trianglesTest\", \"coefficientTest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "training_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Next, train and evaluate the model with these features:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "columns \u003d [\n",
        "    \"cn\", \"pa\", \"tn\", # graph features\n",
        "    \"minTriangles\", \"maxTriangles\", \"minCoefficient\", \"maxCoefficient\" # triangle features  \n",
        "]\n",
        "\n",
        "X \u003d training_df[columns]\n",
        "y \u003d training_df[\"label\"]\n",
        "classifier.fit(X, y)\n",
        "\n",
        "predictions \u003d classifier.predict(test_df[columns])\n",
        "y_test \u003d test_df[\"label\"]\n",
        "\n",
        "display(evaluate_model(predictions, y_test))\n",
        "feature_importance(columns, classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "The coefficient features haven\u0027t added much to the model, but the triangles are useful. Next you will see if community detection algorithms can help improve the model."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "# Community Detection\n\nCommunity detection algorithms evaluate how a group is clustered or partitioned. Nodes are considered more similar to nodes that fall in their community than to nodes in other communities.\n\nYou will use these two community detection algorithms over the train and test sub graphs - Label Propagation and Louvain. \n\nFirst, run Label Propagation: "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.labelPropagation(\"Author\", \"CO_AUTHOR_EARLY\", \"BOTH\",\n",
        "{partitionProperty: \"partitionTrain\"});\n",
        "\"\"\").to_data_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.labelPropagation(\"Author\", \"CO_AUTHOR\", \"BOTH\",\n",
        "{partitionProperty: \"partitionTest\"});\n",
        "\"\"\").to_data_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Next, run Louvain. The Louvain algorithm returns intermediate communities, which are useful for finding fine grained communities that exist in a graph. You will add a property to each node containing the community revealed on the first iteration of the algorithm:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.louvain.stream(\"Author\", \"CO_AUTHOR_EARLY\", {includeIntermediateCommunities:true})\n",
        "YIELD nodeId, community, communities\n",
        "WITH algo.getNodeById(nodeId) AS node, communities[0] AS smallestCommunity\n",
        "SET node.louvainTrain \u003d smallestCommunity;\n",
        "\"\"\").stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "graph.run(\"\"\"\n",
        "CALL algo.louvain.stream(\"Author\", \"CO_AUTHOR\", {includeIntermediateCommunities:true})\n",
        "YIELD nodeId, community, communities\n",
        "WITH algo.getNodeById(nodeId) AS node, communities[0] AS smallestCommunity\n",
        "SET node.louvainTest \u003d smallestCommunity;\n",
        "\"\"\").stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "The following function will add these features to the train and test DataFrames:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def apply_community_features(data, partition_prop, louvain_prop):\n",
        "    query \u003d \"\"\"\n",
        "    UNWIND $pairs AS pair\n",
        "    MATCH (p1) WHERE id(p1) \u003d pair.node1\n",
        "    MATCH (p2) WHERE id(p2) \u003d pair.node2\n",
        "    RETURN pair.node1 AS node1,\n",
        "    pair.node2 AS node2,\n",
        "    algo.linkprediction.sameCommunity(p1, p2, $partitionProp) AS sp,    \n",
        "    algo.linkprediction.sameCommunity(p1, p2, $louvainProp) AS sl\n",
        "    \"\"\"\n",
        "    pairs \u003d [{\"node1\": node1, \"node2\": node2}  for node1,node2 in data[[\"node1\", \"node2\"]].values.tolist()]\n",
        "    params \u003d {\n",
        "    \"pairs\": pairs,\n",
        "    \"partitionProp\": partition_prop,\n",
        "    \"louvainProp\": louvain_prop\n",
        "    }\n",
        "    features \u003d graph.run(query, params).to_data_frame()\n",
        "    return pd.merge(data, features, on \u003d [\"node1\", \"node2\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "training_df \u003d apply_community_features(training_df, \"partitionTrain\", \"louvainTrain\")\n",
        "test_df \u003d apply_community_features(test_df, \"partitionTest\", \"louvainTest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "training_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "columns \u003d [\n",
        "    \"cn\", \"pa\", \"tn\", # graph features\n",
        "    \"minTriangles\", \"maxTriangles\", \"minCoefficient\", \"maxCoefficient\", # triangle features  \n",
        "    \"sp\", \"sl\" # community features\n",
        "]\n",
        "\n",
        "X \u003d training_df[columns]\n",
        "y \u003d training_df[\"label\"]\n",
        "classifier.fit(X, y)\n",
        "\n",
        "predictions \u003d classifier.predict(test_df[columns])\n",
        "y_test \u003d test_df[\"label\"]\n",
        "\n",
        "display(evaluate_model(predictions, y_test))\n",
        "feature_importance(columns, classifier)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}