
= Causal Clustering in Neo4j
:presenter: Neo Technology
:twitter: neo4j
:email: info@neotechnology.com
:neo4j-version: 3.5
:currentyear: 2018
:doctype: book
:toc: left
:toclevels: 3
:experimental:
//:imagedir: https://s3-us-west-1.amazonaws.com/data.neo4j.com/neo4j-admin/img
:imagedir: ../img


++++
	<script type='text/javascript'>
	var loc = window.location;
	if (loc.hostname == "neo4j.com" && loc.search.indexOf("aliId=") == -1 ) {
	 loc.pathname = "/graphacademy/online-training/XXXX/"	
	}
	document.write(unescape("%3Cscript src='//munchkin.marketo.net/munchkin.js' type='text/javascript'%3E%3C/script%3E"));
	</script>
	<script>Munchkin.init('773-GON-065');</script>
++++

== About this module

Now that you have gained experience managing a Neo4j instance and database , you will learn how to get started with creating and managing Neo4j Causal Clusters.

At the end of this module, you should be able to:
[square]
* Describe why you would use Neo4j Causal Clusters.
* Describe the components of a Neo4j Causal Cluster.
* Configure and use a Neo4j Causal Cluster.
* Seed a Causal Cluster with data.
* Monitor and manage core servers in the cluster.
* Monitor and manage read replica servers in the cluster.

This module covers the basics of Causal Clusters in Neo4j. Later in this training, you will learn more about security related to clusters.

== What is Causal Clustering?

Neo4j's Causal Clustering architecture enables an enterprise to utilize a Neo4j database in production. First, it provides a high-available solution whereby if a Neo4j instance has a failure, another Neo4j instance can take over automatically. Secondly, it provides a highly-scalable database whereby some parts of the application update the data, but other parts of the application are widely distributed and do not need immediate access to new or updated data. That is, read latency is acceptable. Causal consistency in Neo4j means that an application is guaranteed to be able to consistently read all data that it has written.

Most Neo4j applications use Causal Clustering to ensure high-availability. The scalability of the Neo4j database is used by applications that utilize multiple data centers.

=== Causal Cluster architecture

There are two types of Neo4j instances in a Causal Clustering architecture: core servers and read replica servers. 

==== Core servers

Core servers are used for read and write access to the database. The core servers are used to synchronize updates to the database, regardless of the number and physical locations of the Neo4j instances. By default, in a Causal Clustering architecture, a transaction is committed if a majority (quorum) of the core servers have written the data to the physical database. This coordination  between core servers is implemented using the Raft protocol. You can have a large number of core servers, but the more core servers in the application architecture, the longer a "majority" commit will take. At a minimum, an application should use three core servers to be considered _fault tolerant_. If you want an architecture that can support two servers failing, then you must configure five core servers. An architecture that is configured with two core servers is not fault tolerant because if one server fails, the second server is automatically set to be read-only.

==== Read replica servers

Read replica servers are used to scale data across a distributed network. They only support read access to the data. The read replica servers regularly poll the core servers for updates to the database by obtaining the transaction log from a core server. You can think of a read replica as a highly scalable and distributed cache of the database.  If a read replica fails, a new read replica can be started with no impact on the data and just a slight impact for the application that can be written to re-connect to a different read replica server.

=== How causal consistency works

An application can create a bookmark that is used to mark the the last transaction committed to the database. In a subsequent read, the bookmark can be used to ensure that the appropriate core servers are used to ensure that only committed data will be read by the application.

== Causal clustering at runtime

As an administrator, you must determine the physical locations of the servers that will be used as core servers and read replica servers.  You configure the casual cluster by updating the *neo4j.conf* file on each server so that they can operate together as a causal cluster. The types of properties that you configure for causal cluster include, but are not limited to:

[square]
* Whether the server will be a core server or a read replica server 
* Public address for the server
* Names/addresses of the servers in the core server membership
* Ports used for communicating between the members
* Published ports for bolt, http, https (non-conflicting port numbers)
* Number of core servers in the causal cluster

=== Core server startup

When a core server starts, it first uses a discovery protocol to join the network. At some point it will be running with the other members of the core membership. In a causal cluster, exactly one core server is eleected to b the _LEADER_. The _LEADER_ is the coordinator of all communication between the core servers. All of the other core servers are _FOLLOWERS_ as the servers in the causal cluster use the raft protocol to synchronize updates.  If a core server joins the network after the other core servers have been running and updating data, the late-joining core server must use the catchup protocol to get to a point where it is synchronized as the other _FOLLOWERS_ are.

=== Core server shutdown

When a core server shuts down, it may be initiated by an administrator, or it may be due to a hardware or network failure. If the core server that is a _FOLLOWER_ shuts down, the _LEADER_ detects and incorporates into its operations with the other core servers. If the core server that is the _LEADER_ shuts down, the remaining core servers communicate with each other and an existing _FOLLOWER_ is promoted to the _LEADER_. If a core server shutdown leaves the causal cluster below a configured threshold for the number of core servers required for the cluster, then the _LEADER_ becomes inoperable for writing to the database. This is a serious matter that needs to be addressed by you as the administrator.

=== Core server updates database

A core server updates its database based upon the requests from clients. The client's transaction is not complete until a quorum of core servers have updated their databases. Subsequent to the completion of the transaction, the remaining core servers will also be updated. Core servers use a _raft protocol_ to share updates. Application clients can use the _bolt_ protocol to send updates to a particular core server's database, but the preferred protocol for an cluster is the _bolt+routing_ protocol. With this protocol, applications can write to any core server in the cluster, but the _LEADER_ will always coordinate updates.

=== Read replica server startup

There can be many read replica servers in a causal cluster. When they start, they register with a core server that maintains a shared whiteboard (cache) that can be used by multiple read replica servers. The read replicas do not use the _raft protocol_. Instead they poll the core servers to obtain the updates to the database that they must apply locally.

=== Read replica server shutdown

Since the read replica servers are considered "transient", when they shut down, there is no effect to the operation of the causal cluster. Of course, detection of a shutdown when it is related to a hardware or network failure must be detected so that a new read replica server can be started so that clients that depend on read access can continue their work.

== Initial administrative tasks for causal clustering

Here are some common tasks for managing and monitoring causal clustering:

. Modify the *neo4j.conf* files for each core server.
. Start the core servers in the causal cluster.
. Seed the core server (add initial data).
. Ensure each core server has the data.
. Modify the *neo4j.conf* files for each read replica server.
. Start the read replica servers.
. Ensure each read replica server has the data.
. Test updates to the database.

In your real application, you  set up each core and read replica Neo4j instances on separate physical servers that are networked and where you have installed Enterprise Edition of Neo4j. In a real application, [underline]#all# configuration for causal clustering is done by modifying the *neo4j.conf* file.

=== Basic core server settings

Please refer to the https://neo4j.com/docs/operations-manual/3.5/clustering/settings/[Neo4j Operations Manual] for greater detail about the settings for configuring causal clustering.

When setting up causal clustering, you should first identify at least three machines that will host core servers. For these machines, you should make sure these properties are set in *neo4j.conf* where XXXX is the IP address of the machine on the network and XXX1, XXX2, XXX3 are the IP addresses of the machines that will participate in the cluster.

----
dbms.connectors.default_listen_address=0.0.0.0

dbms.connector.https.listen_address=0.0.0.0:7473
dbms.connector.http.listen_address=0.0.0.0:7474
dbms.connector.bolt.listen_address=0.0.0.0:7687

dbms.connector.bolt.advertised_address=localhost:18687 ????? what do we do on a real system

causal_clustering.transaction_listen_address=0.0.0.0:6000
causal_clustering.transaction_advertised_address=XXXX:6000
causal_clustering.raft_listen_address=0.0.0.0:7000
causal_clustering.raft_advertised_address=XXXX:7000
causal_clustering.discovery_listen_address=0.0.0.0:5000
causal_clustering.discovery_advertised_address=XXXX:5000

causal_clustering.minimum_core_cluster_size_at_formation=3
causal_clustering.minimum_core_cluster_size_at_runtime=3
causal_clustering.initial_discovery_members=XXX1:5000,XXX2:5000,XXX3:5000,XXX4:5000,XXX5:5000 

dbms.mode=CORE 
----

The minimum number of core servers in a fault-tolerant causal cluster is three. If you require more than three core servers, you must adjust the values in the causal clustering configuration section where you specify the size and the members of the cluster. A best practice is to specify more members in the cluster. This will enable you to later add core servers to the cluster. 

=== Starting the core servers

After you have modified the *neo4j.conf* files for the cluster, you start each Neo4j instance. When you start a set of core servers, it doesn't matter what order they are started. One of the members of the core group will automatically be elected at the _LEADER_. You should observe the log output for each core server instance to ensure that it started with no errors. 

[NOTE]
There is a configuration property (_causal_clustering.refuse_to_be_leader_) that you can set to true in the *neo4j.conf* file that specifies that this particular core server will [under]#never# be a leader. You may want to do this a host system that is not located close to the applications that use it.

=== Viewing the status of the cluster

After you have started the servers in the cluster, you can access status information about the cluster from `cypher-shell` on any of the core servers in the cluster. You simply enter `CALL dbms.cluster.overview();` and it returns information about the servers in the cluster, specifically, which ones are followers and which one is the leader.

image::{imagedir}/clusterOverview.png[clusterOverview,width=800,align=center]

=== Using the Neo4j Enterprise Edition Docker image for this training

For this training, you will gain experience managing and monitoring causal clustering using Docker. You will create and run Docker containers using a Neo4j Enterprise Docker image. This will enable you to start and manage multiple Neo4j instances used for causal clustering on your local machine. 
The published Neo4j Enterprise Edition 3.5.0 Docker image (from DockerHub.com) is pre-configured so that its instances can be easily replicated in a Docker environment that uses causal clustering. Using a Docker image, you create Docker containers that run on your local system. Each Docker container is a Neo4j instance. 

For example, here are the settings in the *neo4j.conf* file for the Neo4j instance container named _core3_ when it starts as a Docker container:

----
#********************************************************************
# Other Neo4j system properties
#********************************************************************
dbms.jvm.additional=-Dunsupported.dbms.udc.source=tarball
wrapper.java.additional=-Dneo4j.ext.udc.source=docker
ha.host.data=core3:6001
ha.host.coordination=core3:5001
dbms.tx_log.rotation.retention_policy=100M size
dbms.memory.pagecache.size=512M
dbms.memory.heap.max_size=512M
dbms.memory.heap.initial_size=512M
dbms.connectors.default_listen_address=0.0.0.0
dbms.connector.https.listen_address=0.0.0.0:7473
dbms.connector.http.listen_address=0.0.0.0:7474
dbms.connector.bolt.listen_address=0.0.0.0:7687
causal_clustering.transaction_listen_address=0.0.0.0:6000
causal_clustering.transaction_advertised_address=core3:6000
causal_clustering.raft_listen_address=0.0.0.0:7000
causal_clustering.raft_advertised_address=core3:7000
causal_clustering.discovery_listen_address=0.0.0.0:5000
causal_clustering.discovery_advertised_address=core3:5000
EDITION=enterprise
ACCEPT.LICENSE.AGREEMENT=yes
----

Some of these settings are for applications that use the _high availability (ha)_ features of Neo4j. With causal clustering, we use the core servers for fault tolerance rather than the high availability features of Neo4j. The setting _dbms.connectors.default_listen_address=0.0.0.0_ is important. This setting enables the instance to communicate with other applications and servers in the network. Notice that the instance has a number of _causal_clustering_ settings that are pre-configured. These are default settings for causal clustering that you can override when you create the Docker container for the first time. Some of the other default settings are recommended settings for a Neo4j instance, whether it is part of a causal cluster or not.  

When you create Docker Neo4j containers using `docker run`, you specify additional causal clustering configuration as parameters, rather than specifying them in the *neo4j.conf* file. Here is an example of the parameters that are specified when creating the Docker container named _core3_ in a script:

----
docker run --name=core3 \
        --volume=`pwd`/core3/conf:/conf --volume=`pwd`/core3/data:/data --volume=`pwd`/core3/logs:/logs  \
        --publish=13474:7474 --publish=13687:7687 \
 	    --env=NEO4J_dbms_connector_bolt_advertised__address=localhost:13687 \
        --network=training-cluster \
        --env=NEO4J_ACCEPT_LICENSE_AGREEMENT=yes  \
	    --env=NEO4J_causal__clustering_minimum__core__cluster__size__at__formation=3 \
        --env=NEO4J_causal__clustering_minimum__core__cluster__size__at__runtime=3 \
        --env=NEO4J_causal__clustering_initial__discovery__members=core1:5000,core2:5000,core3:5000,core4:5000,core5:5000 \
        --env=NEO4J_dbms_mode=CORE \
	   --detach \
        b4ca2f886837
----

In this example, the name of the Docker container is _core3_. We map the conf, data, and logs folders for the Neo4j instance when it starts to our local filesystem. We map the http and bolt ports to values that will be unique on our system (13474 and 13687). We specify the bolt address to use. The name of the Docker network that is used for this cluster is _training-cluster_. _ACCEPT_LICENSE_AGREEMENT_ is required. The size of the cluster is three core servers and the names of the [potential] members are specified as _core1_, _core2_, , _core3_, core4_, and _core5_. These servers use port 5000 for the discovery listen address. This instance will be used as a core server. The container is started in this script detached, meaning that no output or interaction will be produced. And finally the ID of the Neo4j Enterprise 3.5.0 container is specified.

===  *Exercise #1: Getting started with causal clustering*

In this Exercise, you will gain experience with a simple causal cluster using Docker containers.  You will [underline]#not# use Neo4j instances running on your system, but rather Neo4j instances running in Docker containers.

*Before you begin*

. Ensure that Docker Desktop (MAC/Windows) or Docker CE (Debian) is installed (`docker --version`). Here is information about https://hub.docker.com/search/?type=edition&offering=community[downloading and installing Docker].
. Download the file https://s3-us-west-1.amazonaws.com/data.neo4j.com/admin-neo4j/neo4j-docker.zip[neo4j-docker.zip] and unzip it to a folder that will be used to saving Neo4j configuration changes for causal clusters. This will be your working directory for the causal cluster Exercises in this training. *Hint:* `curl -O https://s3-us-west-1.amazonaws.com/data.neo4j.com/admin-neo4j/neo4j-docker.zip`
. Download the Docker image for Neo4j ( `docker pull neo4j:3.5.0-enterprise`).
. Ensure that your user ID has docker privileges: `sudo usermod -aG docker <username>`. You will have to log in and log out to use the new privileges.

*Exercise steps*:

. Open a terminal on your system.
. Confirm that you have the Neo4j 3.5.0 Docker image: `docker images` 

image::{imagedir}/L03-Ex1-DockerImages.png[L03-Ex1-DockerImages,width=800,align=center]

[start=3]
. Navigate to the neo4j-docker folder. This is the folder that will contain all configuration changes for the Neo4j instances you will be running in the cluster. Initially, you will be working with three core servers. Here you can see that you have a folder for each core server and each read replica server.
. Examine the *create_initial_cores.sh* file. This script creates the network that will be used in your Docker environment and then creates three Docker container instances from the Neo4j image. Each instance will represent a core server. Finally, the script stop the three instances.

image::{imagedir}/L03-Ex1-create_cores.png[L03-Ex1-create_cores,width=800,align=center]

[start=5]
. Run *create_initial_cores.sh* as root `sudo ./create_initial_cores.sh <Image ID>` providing as an argument the Image ID of the Neo4j Docker image. 

image::{imagedir}/L03-Ex1-create_cores-run.png[L03-Ex1-create_cores-run,width=800,align=center]

[start=6]
. Confirm that the three containers exist: `docker ps -a`

image::{imagedir}/L03-Ex1-containersCreated.png[L03-Ex1-containersCreated,width=800,align=center]

[start=7]
. Open a terminal window for each of the core servers. (three of them)
. In each core server window, start the instance: `docker start -a coreX`. The instance should be started. These instances are set up so that the default browser port on localhost will be 11474, 12474, and 13474. Notice that each instance uses it's own database as the active database. For example, here is the result of starting the core server containers. Notice that each server starts as part of the cluster. The servers are not fully started until all catchup has been done between the servers and the _Started_ record is shown. The databases will not be accessible by clients until _all_ core members of the cluster have successfully started.

image::{imagedir}/L03-Ex1-CoresStarted.png[L03-Ex1-CoresStarted,width=800,align=center]

[start=9]
. In your non-core server terminal window, confirm that all core servers are running in the network by typing `docker ps -a`.

image::{imagedir}/L03-Ex1-AllCoreServersStarted.png[L03-Ex1-AllCoreServersStarted,width=800,align=center]

[start=10]
. In your non-core server terminal window, log in to the core1 server with `cypher-shell` as follows `docker exec -it core1 /var/lib/neo4j/bin/cypher-shell -u neo4j -p neo4j`
. Change the password. Here is an example where we change the password for core1:

image::{imagedir}/L03-Ex1-ChangePassword.png[L03-Ex1-ChangePassword,width=800,align=center]

[start=12]
. repeat the previous two steps for core2 and core3 to change the password for the _neo4j_ user.
. Log in to any of the servers and get the cluster overview information in `cypher-shell`. In this image, _core1_ is the _LEADER_:

image::{imagedir}/L03-Ex1-ClusterOverview.png[L03-Ex1-ClusterOverview,width=800,align=center]

[start=14]
. Shut down all core servers by typing this in a non-core server terminal window: `docker stop core1 core2 core3`

image::{imagedir}/L03-Ex1-StopCores.png[L03-Ex1-StopCores,width=800,align=center]

[start=15]
. You can now close the terminal windows you used for each of the core servers,  but keep the non-core server window open.


You have now successfully configured, started, and accessed core servers (as Docker containers) running in a causal cluster.

== Seeding the data for the cluster

When setting up a causal cluster for your application, you must ensure that the database that will be used in the cluster has been populated with your application data. Recall that in a causal cluster, each Neo4j instance has its own database, but the data in the databases for each core servers is identical. When you set up the data for the cluster, you can do any of the following, but you must do the same on [underline]#each# of the core servers of the cluster to create the production database. Note that the core servers must be down for these tasks. You learned how to do these tasks in the previous module.

* Restore data using an online backup.
* Load data using an offline backup.
* Create data using the import tool and a set of *.csv* files.

If the the amount of application data is relatively small (less than 10M nodes) you can also load *.csv* data into a running core server in the cluster where all core servers are started. This will propagate the data to all databases in the cluster.

===  *Exercise #2: Seeding the cluster databases*

In this Exercise, you will populate the databases in the cluster that you created earlier. Because you are using Docker containers for learning about causal clustering, you cannot perform the normal seeding procedures as you would in your real production environment because when using the Neo4j Docker containers, the Neo4j instance is started. Instead, you will simply start the core servers in the cluster and connect to one of them. Then you will use `cypher-shell` to load the _Movie_ data into the database. 

*Before you begin*

Ensure that you have performed the steps in Exercise 1 where you set up the core servers as Docker containers. Note that you can perform the steps of this exercise in a single terminal window.


*Exercise steps*:

. In a terminal window, start the core servers:  `docker start core1 core2 core3`. This will start the core servers in background mode where the log is not attached to STDOUT. If you want to see what is happening with a particular core server, you can always view the messages in *<coreX>/logs/debug.log*.
. By default, all writes must be performed by the _LEADER_ of the cluster.  Determine which core server is the _LEADER_. *Hint:* You can do this by logging in to any core server that is running (`docker exec -it <core server> /bin/bash`) and entering the following command: `echo "CALL dbms.cluster.overview();" | /var/lib/neo4j/bin/cypher-shell -u neo4j -p training-helps`. In this example, core1 is the _LEADER_:

image::{imagedir}/L03-Ex2-Core1IsLeader.png[L03-Ex2-Core1IsLeader,width=800,align=center]

[start=3]
. Log in to the core server that is the _LEADER_.
. Run `cypher-shell` specifying that the *movie.cypher* statements will be run. *Hint:* You can do this with a single command line: `/var/lib/neo4j/bin/cypher-shell -u neo4j -p training-helps < /var/lib/neo4j/data/movieDB.cypher`

image::{imagedir}/L03-Ex2-LoadMovieData.png[L03-Ex2-LoadMovieData,width=800,align=center]

[start=5]
. Log in to `cypher-shell` and confirm that the data has been loaded into the database.

image::{imagedir}/L03-Ex2-Data-loaded.png[L03-Ex2-Data-loaded,width=800,align=center]

[start=6]
. Log out of the core server.
. Log in to a _FOLLOWER_ core server with `cypher-shell`. *Hint:* For example, you can log in to core2 with `cypher-shell` with the following command: `docker exec -it core2 /var/lib/neo4j/bin/cypher-shell -u neo4j -p training-helps`
. Verify that the _Movie_ data is in the database for this core server.

image::{imagedir}/L03-Ex2-Core2-loaded.png[L03-Ex2-Core2-loaded,width=800,align=center]

[start=9]
. Log out of the core server.
. Log in to the remaining core server that is the _FOLLOWER_ with `cypher-shell`. 
. Verify that the _Movie_ data is in the database for this core server.

image::{imagedir}/L03-Ex2-Core3-loaded.png[L03-Ex2-Core3-loaded,width=800,align=center]

[start=12]
. Log out of the core server.

You have now seen the cluster in action. Any modification to one database in the core server cluster is propagated to the other core servers. 

== Using the bolt+routing protocol

In a causal cluster, all write operations must be coordinated by the _LEADER_ in the cluster. Which core server is designated as the leader could change at any time in the event of a failure. Applications that access the database can automatically route their write operations to whatever _LEADER_ is available as this functionality is built into the Neo4j driver libraries. To implement the automatic routing, application clients must use the _bolt+routing_ protocol when they connect to any of the core servers in the cluster. 

For example, if you have a cluster with three core servers and _core1_ is the _LEADER_, your application can only write to _core1_ using the _bolt_ protocol and bolt port for _core1_. An easy way to see this restriction is if you use the default address for `cypher-shell` on the system where a _FOLLOWER_ is running. If you connect via `cypher-shell` to the server on _core2_ and attempt to update the database, you receive an error:

image::{imagedir}/CannotWriteFollower.png[CannotWriteFollower,width=800,align=center]

When using causal clustering, [underline]#all# application code should use the _bolt+routing_ protocol which will enable applications to be able to write to the database, even in the event of a failure.

===  *Exercise #3: Accessing the databases in a cluster

In this Exercise, you gain some experience with _bolt+routing_ by running two stand-alone Java applications: one that reads from the database and one that writes to the database.

*Before you begin*

. Ensure that you have performed the steps in Exercise 2 where you have populated the database used for the cluster and all three core servers are running. Note that you can perform the steps of this exercise in a single terminal window.
. Ensure that the three core servers are started.
. Log out of the core server if you have not done so already. You should be in a terminal window where you manage Docker.


*Exercise steps*:

. Navigate to the *neo4j-docker/testApps* folder. 
. There are three Java applications as well as scripts for running them. These scripts enable you to run a read-only client or write client against the database where you specify the protocol and the port for connecting to the Neo4j instance. Unless you modified port numbers in the *create_initial_cores.sh* script when you created the containers, the bolt ports used for core1, core2, and core3 are 11687, 12687, and 13687 respectively. What this means is that clients can read from the database using these ports using the _bolt_ protocol. Try running *testRead.sh*, providing bolt as the protocol and one of the above port numbers. For example, type `./testRead.sh bolt 12687`. You should be able to successfully read from each server. Here is an example of running the script against the core2 server which currently is a _FOLLOWER_ in the cluster:

image::{imagedir}/L03-Ex3_ReadFollower.png[L03-Ex3_ReadFollower,width=800,align=center]

[start=3]
. Next, run the script against the other servers in the network. All reads should be successful.
. Next, run the *testWrite.sh* script against the same port using the _bolt_ protocol. For example, type `./testWrite.sh bolt 11687`. What you should see is that you can only use the _bolt_ protocol for writing against the _LEADER_.

image::{imagedir}/L03-Ex3_WriteLeaderFollower.png[L03-Ex3_WriteLeaderFollower,width=800,align=center]

[start=5]
. Next, change the protocol from _bolt_ to _bolt+routing_ and write to the core servers that are _FOLLOWER_ servers.  For example, type `./testWrite.sh bolt+routing 12687`. With this protocol, all writes are routed to the _LEADER_ and the application can write to the database.

image::{imagedir}/L03-Ex3_AllCanWriteLeader.png[L03-Ex3_AllCanWriteLeader,width=800,align=center]

[start=6]
. Next, you will add data to the database with a client that sends the request to a _FOLLOWER_ core server. Run the *addPerson.sh* script against any port  representing a _FOLLOWER_. using the _bolt_ protocol. For example, type `./addPerson.sh bolt+routing 13687 "Willie"`. This will add a _Person_ node to the database for core3.

image::{imagedir}/L03-Ex3_AddPerson.png[L03-Ex3_AddPerson,width=800,align=center]

[start=7]
. Verify that this newly-added _Person_ node is written to the other servers in the cluster by using the _bolt_ protocol to request specific servers. For example, type `./readPerson.sh bolt 12687 "Willie"` to confirm that the data was added to core2.

image::{imagedir}/L03-Ex3_ReadPerson.png[L03-Ex3_ReadPerson,width=800,align=center]


You have now seen how updates to the core servers in a cluster must be coordinated by the server that is currently the _LEADER_ and how reads and writes are performed in a causal cluster using the _bolt_ and _bolt+routing_ protocols.

== Core server lifecycle

By default, the minimum number of core servers in a cluster is three. When you are setting up your systems for causal clustering, you need to plan which systems will host the core servers. In a causal cluster, not all core servers need to be running, but a minimum number of them [underline]#must# be running. In order for a cluster to be fault-tolerant, at most two core servers must be running. If you start a cluster with three core servers and one of the servers goes stops (either planned or due to a failure), the database is still updatable from clients (provided they use the _bolt+routing_ protocol). If, however, the cluster has a single core server running, the server will be a _FOLLOWER_ and will read-only until one of the other core servers is running.

If the _LEADER_ core server stops, then one of the other _FOLLOWER_ core servers assumes the role of _LEADER_. If a cluster has only _FOLLOWER_ core servers, the database is read-only. This would happen if the number of core servers running goes down to one or if the core servers remaining have been configured to never be a _LEADER_. As an administrator, you must ensure that your cluster always has a _LEADER_.

In addition to using Cypher to retrieve the overview state of the cluster, there are also REST APIs for accessing information about a particular server.  For example, you can query the status of the cluster as follows: `curl -u neo4j:training-helps localhost:11474/db/manage/server/causalclustering/status` where the query is made against the core1 server:

image::{imagedir}/RESTStatus.png[RESTStatus,width=800,align=center]

Or if you want to see it a particular server is writable (part of a "healthy" cluster), for example, you can get that information as follows: `curl -u neo4j:training-helps localhost:11474/db/manage/server/causalclustering/writable` where the query is made against the core1 server:

image::{imagedir}/RESTWritable.png[RESTWritable,width=800,align=center]

Using the REST API enables you as an administrator to script checks against the causal cluster to ensure that it is running properly and available to the clients.

=== Adding core servers to the cluster

If the cluster is inoperable because there is no _LEADER_, or if you want to increase the number of servers available to the clients, you can start a new core server that is configured to be part of the membership in the cluster. That is, all of the core servers must know about the core server (part of member list) and the port numbers used for the new core server must not conflict with the other core servers in the cluster.

Suppose that the cluster has a single core server, core3 running. This makes the database inoperable for updates. 

image::{imagedir}/OneCore.png[OneCore,width=800,align=center]

If we were to start core1 or core2, then the database would be writable because two core servers are now running. If for some reason, core1 and core2 cannot be started, we could start a new core server, core4 that is part of the configured membership. When a new core servers starts and join the cluster, they it need to catch up its data with the other database in the cluster before the cluster is operable for writes. Then one of the core servers will then be elected to be the _LEADER_.

===  *Exercise #4: Adding core servers

In this Exercise, you gain some experience monitoring the cluster and adding core servers so that the database is operable for writing.

*Before you begin*

Ensure that you have performed the steps in Exercise 3 and you have a cluster with core1, core2, ns core3 started.


*Exercise steps*:

. Stop core1 and core2: `docker stop core1 core2`.
. View the cluster overview using core3: `docker exec -it core3 /var/lib/neo4j/bin/cypher-shell -u neo4j -p training-helps  "CALL dbms.cluster.overview();"`. Do you see that core1 is the only server in the cluster and it has the _FOLLOWER_ role?

image::{imagedir}/L03-Ex4_OneCore.png[L03-Ex4_OneCore,width=800,align=center]

. Start core2.
. View the cluster overview using either core2 or core3. Is there a _LEADER_?

image::{imagedir}/L03-Ex4_TwoCores.png[L03-Ex4_OneCore,width=800,align=center]

. Stop core2.
. Next, you will create a new core server container, core4. Run the script for creating and starting the container, providing as input to the script, the ImageID of the Neo4j Docker image.



== Summary

You should now be able to:

[square]

