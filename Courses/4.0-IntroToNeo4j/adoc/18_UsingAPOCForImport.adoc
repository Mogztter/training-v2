= Using APOC for Import

:presenter: Neo Technology
:twitter: neo4j
:email: info@neotechnology.com
:neo4j-version: 4.0
:currentyear: 2020
:doctype: book
:toc: left
:toclevels: 3
:prevsecttitle: Using LOAD CSV for Import
:currsect: 18
:nextsecttitle: Using the neo4j-admin tool for Import
:nextsect: 19
:experimental:
:imagedir: ../img
//:imagedir: https://s3-us-west-1.amazonaws.com/data.neo4j.com/intro-neo4j/img
:manual: http://neo4j.com/docs/developer-manual/current
:manual-cypher: {manual}/cypher

ifdef::backend-html5[]

include::scripts.txt[]

endif::backend-html5[]

== About this module

You have just learned how import data into the graph using Cypher's `LOAD CSV` clause.
This is one of the easiest ways to import data, but it has its limitations.
Next, you will learn how you can use some of the APOC procedures to help you import data into the graph.

At the end of this module, you should be able to:

[square]
* Ensure the APOC library is available.
* Clear a graph of constraints, indexes, nodes, and relationships using APOC.
* Perform conditional processing with APOC during import.
* Use APOC for importing a CSV file.
* Use APOC for importing a JSON file.


== Installing APOC for use with your graph(s)

If you are using a Neo4j Sandbox or Neo4j Aura, your database already has access to the APOC library.
If you are using Neo4j Desktop, you must add the APOC library to your database server you are using.
Here are the steps for adding the APOC library to your database server in Neo4j Desktop:

. Stop the database server.
. For the project you are working with, select "+" in the Add Plugin panel.
. Select APOC to install.
. Close the install panel.
. Start the database server.

You can confirm in Neo4j Browser that you have APOC available by executing this code:

[source,cypher]
----
CALL dbms.procedures()
YIELD name WHERE name STARTS WITH "apoc"
RETURN name
----

[.thumb]
image::{imagedir}/APOCInstalled.png[APOCInstalled,width=900]

== Using APOC to clear the graph

When you are developing code to import data, you may have several attempts to perform the import correctly.
Rather than creating a new database for each attempt, you can completely clear the database of all constraints, indexes, nodes, and relationships.

Here is one way that you can clear the database:
[source,cypher]
----
// Delete all constraints and indexes
CALL apoc.schema.assert({},{},true);
// Delete all nodes and relationships
CALL apoc.periodic.iterate(
  'MATCH (n) RETURN n',
  'DETACH DELETE n',
  { batchSize:500 }
)
----

[.thumb]
image::{imagedir}/ClearDatabase.png[ClearDatabase,width=900]

[NOTE]
In Neo4j 4.0, another way that you can clear the graph in Neo4j Browser is to use the _system_ database and then type `CREATE OR REPLACE DATABASE <database-name>`.

== Using APOC during import

One benefit of using APOC for loading data into the graph is that it can sometimes be faster than LOAD CSV.
In addition, APOC has some procedures that are helpful during the load, one of which is to control conditional processing.
And as you have already learned, with APOC, you can load large datasets that will fail if using `LOAD CSV` or even `USING PERIODIC COMMIT LOAD CSV`.

Just as you inspect the data, determine if data needs to be transformed, and create uniqueness constraints before the import with `LOAD CSV`,
you must do the same when using APOC for the import.

Here is an example of the various types of loading procedures you can use in APOC:

[.thumb]
image::{imagedir}/APOCLOADProcedures.png[APOCLOADProcedures,width=900]


== Using APOC for conditional processing

In the previous lesson, we used `LOAD CSV` to load Movie and Person data into the graph and then use the additional CSV files to create the relationships between the nodes.
Those files represented normalized data where each file basically represents a relational table.

If you want to load denormalized data from a CSV file, you face a couple of challenges.
Just as a reminder, here is a snippet of a denormalized CSV file:

[.thumb]
image::{imagedir}/DenormalizedData.png[DenormalizedData,width=900]

To load this data into the graph you could:

. Make a pass through the file to load the _Movie_ nodes.
. Make a pass through the file to load the _Person_ nodes.
. Make a pass through the file to create relationships *based upon* the _personType_ field.

If the CSV files are large, making multiple passes might not be ideal if you have load time constraints.
A better option might be to:

. Make a pass through the file to load the _Movie_ nodes, collect the person data and then add the _Person_ nodes from the collection.
. Use the person data to create relationships *based upon* the _personType_ field.

Assuming that we will use the second option for importing the data and we have created the uniqueness constraints as before, here is the Cypher code to create the _Person_ and _Movie_ nodes:

[source,cypher]
----
// create constraints
CREATE CONSTRAINT UniqueMovieIdConstraint ON (m:Movie) ASSERT m.id IS UNIQUE;

CREATE CONSTRAINT UniquePersonIdConstraint ON (p:Person) ASSERT p.id IS UNIQUE;

// import the people and movie data (partial; no relationships)
LOAD CSV WITH HEADERS FROM
     'https://data.neo4j.com/v4.0-intro-neo4j/movies2.csv' AS row
WITH row.movieId as movieId, row.title AS title, row.genres AS genres, toInteger(row.releaseYear) AS releaseYear, toFloat(row.avgVote) AS avgVote,
collect({id: row.personId, name:row.name, born: toInteger(row.birthYear), died: toInteger(row.deathYear),personType: row.personType, roles: split(coalesce(row.characters,""),':')}) AS personData
MERGE (m:Movie {id:movieId})
   ON CREATE SET m.title=title, m.avgVote=avgVote,
      m.releaseYear=releaseYear, m.genres=split(genres,":")
WITH *
UNWIND personData AS person
MERGE (p:Person {id: person.id})
   ON CREATE SET p.name = person.name, p.born = person.born, p.died = person.died
----

This code reads the data from a _row_ and creates the _personData_ collection that holds the data for a person.
It creates the _Movie_ nodes based upon the _row_ data.
With the `WITH *" clause, all variables are carried forward in the query.
Then the _personData_ collection is unwound so that each element in a row can be used to create the _Person_ nodes.
Everything is in the graph, except for the relationships.

[.thumb]
image::{imagedir}/APOC-noRelationships.png[APOC-noRelationships,width=900]

This is not quite what we want because we have not created the relationships.
That is, the type of relationship created depends on the value of the _personType_ field in each row of the CSV file.
This is where APOC can help  you.
APOC has a procedure that will allow you to perform conditional execution, based upon a value.

Here is the complete code that utilizes the `apoc.do.when()` procedure, assuming that we have cleared the data from the graph first, but the constraints are still defined.:

[source,cypher]
----
// import the people and movie data and create relationships

LOAD CSV WITH HEADERS FROM
     'https://data.neo4j.com/v4.0-intro-neo4j/movies2.csv' AS row
WITH row.movieId as movieId, row.title AS title, row.genres AS genres, toInteger(row.releaseYear) AS releaseYear, toFloat(row.avgVote) AS avgVote,
collect({id: row.personId, name:row.name, born: toInteger(row.birthYear), died: toInteger(row.deathYear),personType: row.personType, roles: split(coalesce(row.characters,""),':')}) AS people
MERGE (m:Movie {id:movieId})
   ON CREATE SET m.title=title, m.avgVote=avgVote,
      m.releaseYear=releaseYear, m.genres=split(genres,":")
WITH *
UNWIND people AS person
MERGE (p:Person {id: person.id})
   ON CREATE SET p.name = person.name, p.born = person.born, p.died = person.died
// continue processing and use the personType to create the relationships
WITH  m, person, p
CALL apoc.do.when(person.personType = 'ACTOR',
     "MERGE (p)-[:ACTED_IN {roles: person.roles}]->(m)
                ON CREATE SET p:Actor",
     "MERGE (p)-[:DIRECTED]->(m)
         ON CREATE SET p:Director",
     {m:m, p:p, person:person}) YIELD value
RETURN count(*)  // cannot end query with this type of APOC call
----

After the _Movie_ and _Person_ nodes are created, we use the reference to them to create the relationships between them.
The first argument to `apoc.do.when() is the data that is tested.
The second argument is the Cypher code to execute if the test returns true.
The third argument is the Cypher code to execute if the test returns false.
The last argument is the object that describes the mapping of variables both outside of the call and inside the call.
For simplicity, we specify the same values.
Certain `apoc` calls cannot end a Cypher query so we place a `RETURN count(*)` at the end.

Here is the result:

[.thumb]
image::{imagedir}/DoWhenAPOC.png[DoWhenAPOC,width=900]

== Using APOC to import from CSV

If you cannot load the CSV file with `LOAD CSV` or `USING PERIODIC COMMIT LOAD CSV`, another option is to use APOC for the import.
Previously, you learned how to clear the data from the graph using `apoc.periodic.iterate()`.
You use this procedure to load large datasets.

Here is an example with an empty database, but with the constraints defined for the _Person.id_ and _Movie.id_ properties:

[source,cypher]
----
CALL apoc.periodic.iterate(
"CALL apoc.load.csv('https://data.neo4j.com/v4.0-intro-neo4j/movies2.csv' ) YIELD map AS row RETURN row",
"WITH row.movieId as movieId, row.title AS title, row.genres AS genres, toInteger(row.releaseYear) AS releaseYear, toFloat(row.avgVote) AS avgVote,
 collect({id: row.personId, name:row.name, born: toInteger(row.birthYear), died: toInteger(row.deathYear),personType: row.personType, roles: split(coalesce(row.characters,''),':')}) AS people
 MERGE (m:Movie {id:movieId})
    ON CREATE SET m.title=title, m.avgVote=avgVote,
       m.releaseYear=releaseYear, m.genres=split(genres,':')
 WITH *
 UNWIND people AS person
 MERGE (p:Person {id: person.id})
    ON CREATE SET p.name = person.name, p.born = person.born, p.died = person.died
 WITH  m, person, p
 CALL apoc.do.when(person.personType = 'ACTOR',
      'MERGE (p)-[:ACTED_IN {roles: person.roles}]->(m)
                 ON CREATE SET p:Actor',
      'MERGE (p)-[:DIRECTED]->(m)
          ON CREATE SET p:Director',
      {m:m, p:p, person:person}) YIELD value AS value
       RETURN count(*)  ",
{batchSize: 500}
)
----

The first argument to `apoc.periodic.iterate()` is the call to `apoc.load.csv()` where we provide the file name and it returns a _row_.
The second argument is the same Cypher code you saw earlier.
The only thing that is different is that you must ensure that the code is in double quotes and the Cypher code does not use double-quotes (or visa versa).
The final argument is the size of the batch, 500.

Here is the result:

[.thumb]
image::{imagedir}/APOCPeriodicIterate.png[APOCPeriodicIterate,width=900]


== Using APOC to load JSON

JSON is another data format you might need to import into a graph.
There are many data sources out there that can provide data in JSON format.
For this course, we will use the StackOverflow data.
Your first step should be to become familiar with the data that you want to load into the graph.

In this example we call `apoc.load.json` to return 10 questions from StackOverflow so we can view them:

[source,cypher]
----
WITH "https://api.stackexchange.com/2.2/search?page=1&pagesize=5&order=asc&sort=creation&tagged=neo4j&site=stackoverflow&filter=!5-i6Zw8Y)4W7vpy91PMYsKM-k9yzEsSC1_Uxlf" AS uri
CALL apoc.load.json(uri)
YIELD value AS data
UNWIND data.items as q
RETURN q
----

We specify _pagesize_, 5 in the URI. This retrieves 5 questions.
We then `UNWIND` the data and return each question, _q_.

Here is the result:

[.thumb]
image::{imagedir}/ExamineJSONData.png[ExamineJSONData,width=900]

To help you understand the types of data available for each question, you can return the keys for each row:

[source,cypher]
----
WITH "https://api.stackexchange.com/2.2/search?page=1&pagesize=5&order=asc&sort=creation&tagged=neo4j&site=stackoverflow&filter=!5-i6Zw8Y)4W7vpy91PMYsKM-k9yzEsSC1_Uxlf" AS uri
CALL apoc.load.json(uri)
YIELD value AS data
UNWIND data.items as q
RETURN keys(q)
----

We specify _pagesize_, 5 in the URI. This retrieves 5 questions.
We then `UNWIND` the data and return each question, _q_.

Here is the result:

[.thumb]
image::{imagedir}/ExamineJSONDataKeys.png[ExamineJSONDataKeys,width=900]

Next, you must determine what data from the JSON file  you will use to create the graph.

Here we have made a selection for the data we want to create in the graph and we write the code to return it:

[source,cypher]
----
WITH "https://api.stackexchange.com/2.2/search?page=1&pagesize=5&order=asc&sort=creation&tagged=neo4j&site=stackoverflow&filter=!5-i6Zw8Y)4W7vpy91PMYsKM-k9yzEsSC1_Uxlf" AS uri
CALL apoc.load.json(uri)
YIELD value AS data
UNWIND data.items as q
RETURN q.question_id, q.title, q.tags, q.is_answered, q.owner.display_name
----

Here is the result:

[.thumb]
image::{imagedir}/SOQuestionData.png[SOQuestionData,width=900]

We will use all values, except _owner.display_name_ to create a _Question_ node. We will use _owner.display_name_ to create the _User_ nodes.
Here is the code to create the graph:

[source,cypher]
----
WITH "https://api.stackexchange.com/2.2/search?page=1&pagesize=5&order=asc&sort=creation&tagged=neo4j&site=stackoverflow&filter=!5-i6Zw8Y)4W7vpy91PMYsKM-k9yzEsSC1_Uxlf" AS uri
CALL apoc.load.json(uri)
YIELD value AS data
UNWIND data.items as q
MERGE (question:Question {id: q.question_id})
  ON CREATE SET  question.title = q.title,
                 question.tags = q.tags,
                 question.is_answered = q.is_answered
MERGE (user:User {name: q.owner.display_name})
MERGE (user)-[:ANSWERED]->(question)
----

Here is the result of querying the nodes in the graph after the load:

[.thumb]
image::{imagedir}/SOLoadedGraph.png[SOLoadedGraph,width=900]

If you were to load thousands or more questions, you should ensure that you have created uniqueness constraints on _Question.question_id_ and _User.name_ before you attempt the load.


== *Exercise 17: Using APOC for importing data*

In the query edit pane of Neo4j Browser, execute the browser command: kbd:[:play 4.0-intro-neo4j-exercises]
and follow the instructions for Exercise 17.

[#module-7.quiz]
== Check your understanding
=== Question 1

What APOC procedure can you use to batch transactions when a lot of data needs to be processed?

Select the correct answer.
[%interactive]

- [ ] [.false-answer]#apoc.batch().#
- [ ] [.false-answer]#apoc.transaction.batch().#
- [ ] [.false-answer]#apoc.iterate().#
- [ ] [.required-answer]#apoc.periodic.iterate().#


=== Question 2

The procedure `apoc.do.when()` is used for:

Select the correct answer.
[%interactive]

- [ ] [.false-answer]#Scheduling when a load should occur.#
- [ ] [.required-answer]#Executing Cypher code when a condition is true and alternate Cypher code when the condition is false.#
- [ ] [.false-answer]#An alternative to the MERGE clause.#
- [ ] [.required-answer]#Profiling the execution of a query.#

=== Question 3

What does `CALL apoc.schema.assert({},{},true)` do?

Select the correct answers.
[%interactive]
- [ ] [.required-answer]#Drops all constraints in the graph.#
- [ ] [.required-answer]#Drops all indexes in the graph.#
- [ ] [.false-answer]#Removes all relationships in the graph.#
- [ ] [.false-answer]#Removes all nodes in the graph.#

== Summary

You should now be able to:

[square]
* Ensure the APOC library is available.
* Clear a graph of constraints, indexes, nodes, and relationships using APOC.
* Perform conditional processing with APOC during import.
* Use APOC for importing a CSV file.
* Use APOC for importing a JSON file.


++++
<a class="next-section medium button" href="../part-8/">Continue to Module 8</a>
++++

ifdef::backend-html5[]

include::scripts-end.txt[]

++++
<script>
$( document ).ready(function() {
  Intercom('trackEvent','training-introv2-view-part7');
});
</script>
++++

endif::backend-html5[]
